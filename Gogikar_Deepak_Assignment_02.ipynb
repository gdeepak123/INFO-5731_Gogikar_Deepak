{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_movie_reviews(movie_url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "    response = requests.get(movie_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    review_count = 0\n",
        "\n",
        "\n",
        "    while review_count < 1000:\n",
        "        for review in soup.find_all('div', class_='review-container'):\n",
        "            if review_count >= 1000:\n",
        "                break\n",
        "            review_text = review.find('div', class_='text show-more__control').get_text().strip()\n",
        "            reviews.append(review_text)\n",
        "            review_count += 1\n",
        "\n",
        "        next_button = soup.find('div', class_='load-more-data')\n",
        "        if next_button:\n",
        "            next_page = next_button.get('data-key')\n",
        "            next_url = f'{movie_url}/reviews/_ajax?ref_=undefined&paginationKey={next_page}'\n",
        "            response = requests.get(next_url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return reviews\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Review'])\n",
        "        for item in data:\n",
        "            writer.writerow([item])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example taken URL for movies released in 2023 or 2024\n",
        "    movie_urls = [\n",
        "        \"https://www.imdb.com/title/tt15398776/reviews/?ref_=tt_ql_2\",\n",
        "    ]\n",
        "\n",
        "    all_reviews = []\n",
        "    for movie_url in movie_urls:\n",
        "        reviews = scrape_imdb_movie_reviews(movie_url)\n",
        "        all_reviews.extend(reviews)\n",
        "\n",
        "    save_to_csv(all_reviews, 'hanuman1_imdb_movie_reviews.csv')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "701196bc-e9ba-43aa-a918-07008d47b82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dd213f7a-1bbf-4cf3-9ba9-888c3029a3c6\", \"hanuman1_imdb_movie_reviews_cleaned.csv\", 3250062)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from google.colab import files\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Reading the CSV file\n",
        "df = pd.read_csv(\"hanuman1_imdb_movie_reviews.csv\")\n",
        "\n",
        "# Defining stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to cleaning text\n",
        "def clean_text(text):\n",
        "    # Removing for noise\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Removing for numbers\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "    # Removing for stopwords\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    # Lowercasing  the  all texts data\n",
        "    text = text.lower()\n",
        "    # Stemming\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    # Lemmatization\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Cleaning the text data\n",
        "df['Cleaned_Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "# Saving and downloading the clean data to a new CSV file\n",
        "df.to_csv(\"hanuman1_imdb_movie_reviews_cleaned.csv\", index=False)\n",
        "\n",
        "files.download(\"hanuman1_imdb_movie_reviews_cleaned.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a905d04-58f3-4b3c-a808-3aee124d1a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Nouns: 56\n",
            "Total Verbs: 14\n",
            "Total Adjectives: 5\n",
            "Total Adverbs: 3\n",
            "\n",
            "Named Entities:\n",
            "Speech: 2\n",
            "POS: 1\n",
            "Tag Parts: 1\n",
            "N: 1\n",
            "V: 1\n",
            "Adj: 1\n",
            "Adv: 1\n",
            "Dependency: 1\n",
            "Entity Recognition: 1\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "[('Write', 'VB', 'O'), ('a', 'DT', 'O'), ('python', 'NN', 'O'), ('program', 'NN', 'O'), ('to', 'TO', 'O'), ('conduct', 'VB', 'O'), ('syntax', 'NN', 'O'), ('and', 'CC', 'O'), ('structure', 'NN', 'O'), ('analysis', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('clean', 'JJ', 'O'), ('text', 'NN', 'O'), ('you', 'PRP', 'O'), ('just', 'RB', 'O'), ('saved', 'VBN', 'O'), ('above', 'RB', 'O'), ('.', '.', 'O')]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "[('The', 'DT', 'O'), ('syntax', 'NN', 'O'), ('and', 'CC', 'O'), ('structure', 'NN', 'O'), ('analysis', 'NN', 'O'), ('includes', 'VBZ', 'O'), (':', ':', 'O'), ('(', '(', 'O'), ('1', 'CD', 'O'), (')', ')', 'O'), ('Parts', 'NNS', 'O'), ('of', 'IN', 'O'), ('Speech', 'NNP', 'B-GPE'), ('(', '(', 'O'), ('POS', 'NNP', 'B-ORGANIZATION'), (')', ')', 'O'), ('Tagging', 'NN', 'O'), (':', ':', 'O'), ('Tag', 'NNP', 'B-PERSON'), ('Parts', 'NNP', 'I-PERSON'), ('of', 'IN', 'O'), ('Speech', 'NNP', 'B-GPE'), ('of', 'IN', 'O'), ('each', 'DT', 'O'), ('word', 'NN', 'O'), ('in', 'IN', 'O'), ('the', 'DT', 'O'), ('text', 'NN', 'O'), (',', ',', 'O'), ('and', 'CC', 'O'), ('calculate', 'VB', 'O'), ('the', 'DT', 'O'), ('total', 'JJ', 'O'), ('number', 'NN', 'O'), ('of', 'IN', 'O'), ('N', 'NNP', 'B-GPE'), ('(', '(', 'O'), ('oun', 'NN', 'O'), (')', ')', 'O'), (',', ',', 'O'), ('V', 'NNP', 'B-GPE'), ('(', '(', 'O'), ('erb', 'NN', 'O'), (')', ')', 'O'), (',', ',', 'O'), ('Adj', 'NNP', 'B-GPE'), ('(', '(', 'O'), ('ective', 'JJ', 'O'), (')', ')', 'O'), (',', ',', 'O'), ('Adv', 'NNP', 'B-GPE'), ('(', '(', 'O'), ('erb', 'NN', 'O'), (')', ')', 'O'), (',', ',', 'O'), ('respectively', 'RB', 'O'), ('.', '.', 'O')]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "[('(', '(', 'O'), ('2', 'CD', 'O'), (')', ')', 'O'), ('Constituency', 'NN', 'O'), ('Parsing', 'NNP', 'O'), ('and', 'CC', 'O'), ('Dependency', 'NNP', 'B-ORGANIZATION'), ('Parsing', 'NNP', 'O'), (':', ':', 'O'), ('print', 'NN', 'O'), ('out', 'IN', 'O'), ('the', 'DT', 'O'), ('constituency', 'NN', 'O'), ('parsing', 'VBG', 'O'), ('trees', 'NNS', 'O'), ('and', 'CC', 'O'), ('dependency', 'NN', 'O'), ('parsing', 'VBG', 'O'), ('trees', 'NNS', 'O'), ('of', 'IN', 'O'), ('all', 'PDT', 'O'), ('the', 'DT', 'O'), ('sentences', 'NNS', 'O'), ('.', '.', 'O')]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "[('Using', 'VBG', 'O'), ('one', 'CD', 'O'), ('sentence', 'NN', 'O'), ('as', 'IN', 'O'), ('an', 'DT', 'O'), ('example', 'NN', 'O'), ('to', 'TO', 'O'), ('explain', 'VB', 'O'), ('your', 'PRP$', 'O'), ('understanding', 'VBG', 'O'), ('about', 'IN', 'O'), ('the', 'DT', 'O'), ('constituency', 'NN', 'O'), ('parsing', 'VBG', 'O'), ('tree', 'NN', 'O'), ('and', 'CC', 'O'), ('dependency', 'NN', 'O'), ('parsing', 'VBG', 'O'), ('tree', 'NN', 'O'), ('.', '.', 'O')]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "[('(', '(', 'O'), ('3', 'CD', 'O'), (')', ')', 'O'), ('Named', 'VBN', 'O'), ('Entity', 'NNP', 'B-PERSON'), ('Recognition', 'NNP', 'I-PERSON'), (':', ':', 'O'), ('Extract', 'NNP', 'O'), ('all', 'PDT', 'O'), ('the', 'DT', 'O'), ('entities', 'NNS', 'O'), ('such', 'JJ', 'O'), ('as', 'IN', 'O'), ('person', 'NN', 'O'), ('names', 'NNS', 'O'), (',', ',', 'O'), ('organizations', 'NNS', 'O'), (',', ',', 'O'), ('locations', 'NNS', 'O'), (',', ',', 'O'), ('product', 'NN', 'O'), ('names', 'NNS', 'O'), (',', ',', 'O'), ('and', 'CC', 'O'), ('date', 'NN', 'O'), ('from', 'IN', 'O'), ('the', 'DT', 'O'), ('clean', 'JJ', 'O'), ('texts', 'NN', 'O'), (',', ',', 'O'), ('calculate', 'VBP', 'O'), ('the', 'DT', 'O'), ('count', 'NN', 'O'), ('of', 'IN', 'O'), ('each', 'DT', 'O'), ('entity', 'NN', 'O'), ('.', '.', 'O')]\n",
            "\n",
            "Dependency Parsing Tree:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.parse import DependencyGraph\n",
        "\n",
        "# Sample clean text\n",
        "clean_text = \"\"\"\n",
        "Write a python program to conduct syntax and structure analysis of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenizimg the text into sentences\n",
        "sentences = sent_tokenize(clean_text)\n",
        "\n",
        "# Tokenizing each sentence into words and taging the parts of speech\n",
        "pos_tags = []\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    pos_tags.append(tagged_words)\n",
        "    for word, pos in tagged_words:\n",
        "        if pos.startswith('N'):  # Nouns in sentence\n",
        "            noun_count += 1\n",
        "        elif pos.startswith('V'):  # Verb in sentence\n",
        "            verb_count += 1\n",
        "        elif pos.startswith('J'):  # Adjective in sentence\n",
        "            adj_count += 1\n",
        "        elif pos.startswith('R'):  # Adverb in sentence\n",
        "            adv_count += 1\n",
        "\n",
        "# POS tagging results\n",
        "print(f\"Total Nouns: {noun_count}\")\n",
        "print(f\"Total Verbs: {verb_count}\")\n",
        "print(f\"Total Adjectives: {adj_count}\")\n",
        "print(f\"Total Adverbs: {adv_count}\")\n",
        "entities = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    named_entities = ne_chunk(tagged_words)\n",
        "    for entity in named_entities:\n",
        "        if isinstance(entity, nltk.Tree):\n",
        "            entities.append(' '.join([word for word, pos in entity.leaves()]))\n",
        "\n",
        "#named entities and their counts\n",
        "print(\"\\nNamed Entities:\")\n",
        "for entity, count in nltk.FreqDist(entities).items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "sentences = sent_tokenize(clean_text)\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    print(\"\\nConstituency Parsing Tree:\")\n",
        "    print(nltk.chunk.tree2conlltags(ne_chunk(tagged_words)))\n",
        "\n",
        "    print(\"\\nDependency Parsing Tree:\")\n",
        "    #print(nltk.parse.DependencyGraph.from_iterable(tagged_words).tree())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "# In the above Assignment i wolud encountered to execute the code but with in built libraries it make easy to solve. Even it is type of learning to know\n",
        "#required libraries to perform nlp functions and perform operations given. I have learn to extract and save the data set of customers given reviews for perticular movie\n",
        "# and able to clean data set. I able to comple the assignment on time.\n",
        "#Thank you."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}